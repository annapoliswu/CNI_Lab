<html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet type=text/css href=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css integrity=sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh crossorigin=anonymous><link rel=stylesheet type=text/css href=https://annapoliswu.github.io/CNI_Lab//css/style.css><script src=https://code.jquery.com/jquery-3.4.1.slim.min.js integrity=sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js integrity=sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo crossorigin=anonymous></script><script src=https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js integrity=sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6 crossorigin=anonymous></script></head><body><nav class="navbar navbar-expand-md justify-content-end navbar-dark bg-dark"><a class=navbar-brand href=https://annapoliswu.github.io/CNI_Lab/><img src=https://annapoliswu.github.io/CNI_Lab//images/logo-white-fill.svg height=50px width=auto>
CNI Engineering Lab</a><div class="ml-auto mr-1"></div><button class="navbar-toggler navbar-dark" type=button data-toggle=collapse data-target=#navbarSupportedContent>
<span class=navbar-toggler-icon></span></button><div class="navbar-collapse collapse flex-grow-0" id=navbarSupportedContent><ul class="navbar-nav text-right"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" data-hover=dropdown href=https://annapoliswu.github.io/CNI_Lab/research/ role=button aria-haspopup=true aria-expanded=false>Research</a><div class=dropdown-menu><a class=dropdown-item href=https://annapoliswu.github.io/CNI_Lab/research/testpage/>Quantifying Informativeness</a></div></li><li class=nav-item><a class=nav-link href=https://annapoliswu.github.io/CNI_Lab/members/>Members</a></li><li class=nav-item><a class=nav-link href=https://annapoliswu.github.io/CNI_Lab/singles/admissions/>Admissions</a></li><li class=nav-item><a class=nav-link href=https://annapoliswu.github.io/CNI_Lab/singles/contact/>Contact</a></li></ul></div></nav><div class=container><div id=content><h1>Quantifying the Informativeness of Similarity Measurements</h1><p>Austin J. Brockmeier, Tingting Mu, Sophia Ananiadou, and John Y. Goulermas<br><i>Journal of Machine Learning Research</i>, Vol. 18, No. 18, pp. 1-61, 2017.</p><p><p><div class=md_image_wrapper><img class=md_image src=https://annapoliswu.github.io/CNI_Lab/images/logo-black-fill.svg alt="alt text here" title="title hoverover"></div></p><h2 id=overview>Overview</h2><p>Choosing the particulars of a data representation is crucial for the successful application of machine learning techniques. In the unsupervised case, there is a lack of measures that can be used to compare different parameter choices that affect the representation. In this paper, we describe an unsupervised measure for quantifying the &lsquo;informativeness&rsquo; of correlation matrices formed from the pairwise similarities or relationships among data instances.</p><p>The measure quantifies the heterogeneity of the correlations and is defined as the distance between a correlation matrix and the nearest correlation matrix with constant off-diagonal entries. While a homogenous correlation matrix indicates every instance is the same or equally dissimilar, informative correlation matrices are not uniform, some subsets of instances are more similar and themselves are dissimilar to other subsets. A set of distinct clusters is highly informative (Figure 1).</p><p><div class=md_image_wrapper><img class=md_image src=https://annapoliswu.github.io/CNI_Lab/images/informativeness1.png alt="Figure 1: Informativeness versus von Neumann entropy for correlation matrices obtained from various configurations of four unit vectors. Both measures are minimal when the vectors are configured in a single cluster. Informativeness is higher for nontrivial clusterings, whereas entropy is maximized when the vectors are maximally separated." title="Figure 1"></div></p><p>Informativeness can be used as an function to choose between representations or perform parameter selection (Figure 2) or dimensionality reduction. Using it, we designed a convex optimization algorithm for de-noising correlation matrices that clarifies their cluster structure.</p><p><div class=md_image_wrapper><img class=md_image src=https://annapoliswu.github.io/CNI_Lab/images/informativeness2.png alt="Figure 2: Informativeness versus the von Neumann entropy of correlation matrices obtained from a Gaussian kernel applied with varying bandwidths to a sample with 2 clusters." title="Figure 2"></div></p></p><script>$(".md_image").each(function(){let a=$(this),b=a.attr('alt');a.after(`<figure>${b}</figure>`)})</script><style>h1{font-size:2em}.md_image,figure{width:80%;height:auto;padding:1em 0}.md_image,figure{display:block;margin-left:auto;margin-right:auto}figure{font-size:.8em;font-style:italic}</style></div></div><footer class="footer mt-5"><div class=container><table class="table table-sm"><caption>Hugo Variables for current page</caption><tr><th>Name</th><td>Quantifying the informativeness of similarity measurements</td></tr><tr><th>Kind</th><td>page</td></tr><tr><th>Type</th><td>research</td></tr><tr><th>List Page</th><td>.Pages</td></tr><tr><th>IsPage</th><td>true</td></tr><tr><th>IsHome</th><td>true</td></tr><tr><th>Next</th><td>Page(/singles/contact.md)</td></tr><tr><th>Prev</th><td>Page(/members/current/AndrewBrockmeier.md)</td></tr><tr><th>Section</th><td>Page(/research)</td></tr></table></div></footer></body></html>